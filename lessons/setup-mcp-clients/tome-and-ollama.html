<!DOCTYPE html><html><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png" data-next-head=""/><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png" data-next-head=""/><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png" data-next-head=""/><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png" data-next-head=""/><link rel="icon" type="image/x-icon" href="/images/favicon.ico" data-next-head=""/><title data-next-head="">Tome and Ollama – Complete Intro to MCP</title><meta name="description" content="Brian Holt explains practical options for running tool-enabled LLMs within the MCP framework, including Tome (an open-source chat app supporting OpenAI and Google Gemini) and Ollama for local model hosting, with guidance on selecting tool-capable models and managing GPU/CPU inference costs. It also highlights OpenRouter and the Tome Discord community for easy model switching within MCP." data-next-head=""/><meta name="keywords" content="Tome,Ollama,tools calling,OpenRouter,MCP,AI agents,Brian Holt" data-next-head=""/><meta name="og:description" content="Brian Holt explains practical options for running tool-enabled LLMs within the MCP framework, including Tome (an open-source chat app supporting OpenAI and Google Gemini) and Ollama for local model hosting, with guidance on selecting tool-capable models and managing GPU/CPU inference costs. It also highlights OpenRouter and the Tome Discord community for easy model switching within MCP." data-next-head=""/><meta name="og:title" content="Tome and Ollama – Complete Intro to MCP" data-next-head=""/><meta name="og:image" content="/images/social-share-cover.jpg" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><link data-next-font="size-adjust" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/086f6a648d0d9276.css" as="style"/><link rel="stylesheet" href="/_next/static/css/086f6a648d0d9276.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-b66ed8ba6f029a99.js" defer=""></script><script src="/_next/static/chunks/framework-2f335d22a7318891.js" defer=""></script><script src="/_next/static/chunks/main-00b4e1227fd2aa8a.js" defer=""></script><script src="/_next/static/chunks/pages/_app-b9f8acf1434dd4b3.js" defer=""></script><script src="/_next/static/chunks/pages/lessons/%5Bsection%5D/%5Bslug%5D-2115f22e4e476f99.js" defer=""></script><script src="/_next/static/3Mg5_eXgxRUcnLOJp0h8Y/_buildManifest.js" defer=""></script><script src="/_next/static/3Mg5_eXgxRUcnLOJp0h8Y/_ssgManifest.js" defer=""></script></head><body><script async="" defer="" src="https://a.holt.courses/latest.js"></script><div id="__next"><div class="remix-app"><header class="navbar"><h1 class="navbar-brand"><a href="/">Complete Intro to MCP</a></h1><div class="navbar-info"><a href="https://frontendmasters.com/courses/mcp/" class="cta-btn">Watch on Frontend Masters</a></div></header><div class="content-container"><div class="main"><div class="lesson-container"><div class="lesson"><div class="lesson-content"><p>I wanted to give you a second option if for whatever reason you don&#39;t want to use Claude Desktop. Again, there are a myriad of choices but I was particularly impressed with <a href="https://gettome.app">Tome</a>. Tome is an open source app to chat with LLMs. It supports OpenAI, Google Gemini, and Ollama - so if you want to use either OpenAI or Gemini, here&#39;s the place to do it.</p>
<blockquote>
<p>Note that Tome, only (as of writing), supports tools, not prompts or resources. But honestly tools are the most important and fun anyway.</p>
</blockquote>
<p>You can also run models locally on your computer using <a href="https://ollama.com/">Ollama</a>. Ollama is a very cool piece of software that makes it&#39;s trivial to run models on your computer. It will spin up a local web server for you to make queries against. We can have our Tome then query that locally-running LLM.</p>
<p>Keep in mind that the larger, &quot;smarter&quot; models typically require beefy GPUs to run with any sort of speed. That said, there are some models that run on even just CPUs and can still do tools calling.</p>
<blockquote>
<p>New models are coming out at break-neck pace. By the time you read this versus when I wrote it, I guarantee there will be several cycle of new models available. You will have to select one based on what&#39;s available. The good news is that the new ones are only getting more performant and &quot;smarter&quot;.</p>
</blockquote>
<p>What&#39;s important is that you select a model that can handle <a href="https://ollama.com/search?c=tools">tools calling</a>. As of writing, a few good options that run on less-capable hardware</p>
<ul>
<li><a href="https://ollama.com/library/deepseek-r1">DeepSeek R1 1.5B</a></li>
<li><a href="https://ollama.com/library/qwen3">Qwen3 0.6B</a></li>
<li><a href="https://ollama.com/library/phi4-mini">Phi4-Mini 3.8B</a></li>
<li><a href="https://ollama.com/library/llama3.2">Llama 3.2 1B</a></li>
</ul>
<p>The B means billions, and it&#39;s how many parameters that model has which (<em>very roughly</em>) can translate to how smart it is, sorta like how megahertz <em>very roughly</em> translates to how capable a CPU is – they&#39;re loosely related but you shouldn&#39;t say that the Llama model is smarter than the DeepSeek one just because it has more parameters.</p>
<p>You may need to try a few models before you find one that works well with what you&#39;re doing. Just make sure you&#39;re using tools-calling models as not all of them do it. I at home run Ollama on my gaming PC so I can take advantage of the beefy GPU I have in there - you can definitely do that too since Ollama is just a web server. In that case you can run a larger model as it will fit in VRAM.</p>
<p>Again, keep in mind that running inference on a model is very computationally expensive so you shouldn&#39;t expose it to the world and you should be aware that if you run a lot of inference you can rack up a power bill!</p>
<blockquote>
<p>With Tome it doesn&#39;t show you streaming text while it&#39;s thinking so if you picked a big model that&#39;s a bit slower to run, it can look like it&#39;s not doing anything.</p>
</blockquote>
<p>I asked the creators of Tome if they wanted to share anything with you all and they said if you&#39;re passionate about LLMs, agents, and/or MCP servers to join them on <a href="https://discord.com/invite/9CH6us29YA">Discord</a>!</p>
<blockquote>
<p>I&#39;ll put one shout out in here for <a href="https://openrouter.ai/">OpenRouter.ai</a>. If you want to try a lot of models and make it easy to switch between them, OpenRouter makes that very simple to do. Lots of other services exist (including on Databricks!) but OpenRouter works very well in this use case.</p>
</blockquote>
</div><div class="lesson-links"><a href="/lessons/setup-mcp-clients/claude-desktop" class="prev">← Previous</a><a href="/lessons/setup-mcp-clients/other-clients" class="next">Next →</a></div></div><div class="details-bg"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="154" height="154" viewBox="0 0 154 154"><defs><clipPath id="clip-path"><rect id="Rectangle_2238" data-name="Rectangle 2238" width="154" height="154" transform="translate(9467 350)" fill="#fff" stroke="#707070" stroke-width="1"></rect></clipPath><clipPath id="clip-corner-image-active"><rect width="154" height="154"></rect></clipPath></defs><g id="corner-image-active" clip-path="url(#clip-corner-image-active)"><g id="Corner-image-active-2" data-name="Corner-image-active" transform="translate(-9467 -350)" clip-path="url(#clip-path)"><path id="Subtraction_34" data-name="Subtraction 34" d="M-3857.365,1740.766h0l-7.07-7.07,12.89-12.89v14.142l-5.818,5.818Zm-14.142-14.142h0l-7.071-7.07,27.033-27.033v14.143l-19.96,19.96Zm-14.143-14.143h0l-7.07-7.069,41.175-41.175v14.142Zm-14.142-14.142h0l-7.07-7.069,55.317-55.317v14.142Zm-14.142-14.142h0l-7.07-7.069,69.459-69.459v14.142Zm-14.142-14.142h0l-7.07-7.069,76.739-76.739h6.862v7.28Zm-14.143-14.143h0l-7.07-7.069,62.6-62.6h14.142Zm-14.142-14.142h0l-7.07-7.069,48.454-48.454h14.142Zm-14.142-14.142h0l-7.07-7.069,34.312-34.312h14.142Zm-14.142-14.142h0l-7.07-7.069,20.17-20.17h14.142Zm-14.142-14.142h0l-7.071-7.071,6.027-6.027h14.144l-13.1,13.1Zm367.24-56.114v-.909l.455.455-.453.453Z" transform="translate(13472.546 -1236.766)" fill="var(--corner-fill)"></path></g></g></svg></div></div></div></div><noscript><img src="https://a.holt.courses/noscript.gif" alt="" referrerPolicy="no-referrer-when-downgrade"/></noscript><footer class="footer"><ul class="socials"><li class="social"><a href="https://twitter.com/holtbt"><svg fill="none" height="100%" width="32" xmlns="http://www.w3.org/2000/svg" viewBox="0.254 0.25 500 451.95400000000006"><path d="M394.033.25h76.67L303.202 191.693l197.052 260.511h-154.29L225.118 294.205 86.844 452.204H10.127l179.16-204.77L.254.25H158.46l109.234 144.417zm-26.908 406.063h42.483L135.377 43.73h-45.59z" fill="var(--footer-icons)"></path></svg></a></li><li class="social"><a href="https://bsky.app/profile/brianholt.me"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 -3.268 64 68.414" width="38" height="100%"><path fill="var(--footer-icons)" d="M13.873 3.805C21.21 9.332 29.103 20.537 32 26.55v15.882c0-.338-.13.044-.41.867-1.512 4.456-7.418 21.847-20.923 7.944-7.111-7.32-3.819-14.64 9.125-16.85-7.405 1.264-15.73-.825-18.014-9.015C1.12 23.022 0 8.51 0 6.55 0-3.268 8.579-.182 13.873 3.805zm36.254 0C42.79 9.332 34.897 20.537 32 26.55v15.882c0-.338.13.044.41.867 1.512 4.456 7.418 21.847 20.923 7.944 7.111-7.32 3.819-14.64-9.125-16.85 7.405 1.264 15.73-.825 18.014-9.015C62.88 23.022 64 8.51 64 6.55c0-9.818-8.578-6.732-13.873-2.745z"></path></svg></a></li><li class="social"><a href="https://github.com/btholt"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="32" height="32" viewBox="0 0 32 32"><defs><clipPath id="clip-github-social"><rect width="32" height="32"></rect></clipPath></defs><g id="github-social" clip-path="url(#clip-github-social)"><g id="Group_272" data-name="Group 272" transform="translate(13522.5 -6994)"><path id="Subtraction_33" data-name="Subtraction 33" d="M-24967.5,8041a15.9,15.9,0,0,1-11.312-4.688A15.893,15.893,0,0,1-24983.5,8025a15.893,15.893,0,0,1,4.689-11.315A15.894,15.894,0,0,1-24967.5,8009a15.894,15.894,0,0,1,11.313,4.686A15.893,15.893,0,0,1-24951.5,8025a15.893,15.893,0,0,1-4.689,11.313A15.9,15.9,0,0,1-24967.5,8041Zm-3.781-4.571h0v3.918h7.895v-6.665a1.836,1.836,0,0,0-1.2-1.718c5.1-.617,7.467-2.975,7.467-7.424a7.176,7.176,0,0,0-1.637-4.728,6.74,6.74,0,0,0,.275-1.812,4.34,4.34,0,0,0-.52-2.452.574.574,0,0,0-.359-.1c-1.061,0-3.465,1.411-3.936,1.694a16.644,16.644,0,0,0-4.2-.489,16.379,16.379,0,0,0-3.969.445c-.846-.5-2.91-1.649-3.859-1.649a.566.566,0,0,0-.354.095,4.3,4.3,0,0,0-.521,2.452,6.7,6.7,0,0,0,.244,1.718,7.346,7.346,0,0,0-1.6,4.822,7.263,7.263,0,0,0,1.533,4.985c1.193,1.359,3.115,2.165,5.871,2.464a1.826,1.826,0,0,0-1.129,1.693v.5h0l-.006,0a7.121,7.121,0,0,1-2.033.363,2.608,2.608,0,0,1-.965-.158,4.438,4.438,0,0,1-1.836-1.881,2.361,2.361,0,0,0-1.248-1.091,3.472,3.472,0,0,0-1.217-.3.584.584,0,0,0-.545.224.282.282,0,0,0,.027.367,1.875,1.875,0,0,0,.447.307,4.732,4.732,0,0,1,.561.355,10.726,10.726,0,0,1,1.682,2.755c.043.092.078.163.105.217a3.876,3.876,0,0,0,2.42,1.185,6.036,6.036,0,0,0,.607.025c.875,0,1.988-.124,2-.125Z" transform="translate(11461 -1015)" fill="var(--footer-icons)"></path><g id="Ellipse_670" data-name="Ellipse 670" transform="translate(-13522.5 6994)" fill="none" stroke="var(--footer-icons)" stroke-width="1"><circle cx="16" cy="16" r="16" stroke="none"></circle><circle cx="16" cy="16" r="15.5" fill="none"></circle></g></g></g></svg></a></li><li class="social"><a href="https://linkedin.com/in/btholt"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="32" height="32" viewBox="0 0 32 32"><defs><clipPath id="clip-linkedin-social"><rect width="32" height="32"></rect></clipPath></defs><g id="linkedin-social" clip-path="url(#clip-linkedin-social)"><g id="Group_270" data-name="Group 270" transform="translate(-86.349 -633.073)"><path id="Path_375" data-name="Path 375" d="M115.789,633.073a2.324,2.324,0,0,1,1.682.676,2.194,2.194,0,0,1,.695,1.627V662.8a2.131,2.131,0,0,1-.695,1.609,2.314,2.314,0,0,1-1.646.659H88.69a2.307,2.307,0,0,1-1.646-.659,2.128,2.128,0,0,1-.695-1.609V635.376a2.19,2.19,0,0,1,.695-1.627,2.322,2.322,0,0,1,1.682-.676h27.063Zm-20.224,9.672a2.561,2.561,0,0,0,0-3.584,2.658,2.658,0,0,0-1.938-.712,2.724,2.724,0,0,0-1.957.712,2.371,2.371,0,0,0-.75,1.792,2.4,2.4,0,0,0,.731,1.792,2.605,2.605,0,0,0,1.9.713h.037A2.7,2.7,0,0,0,95.565,642.745ZM96,645.434H91.213V659.88H96Zm17.3,6.144a7.007,7.007,0,0,0-1.573-4.9,5.68,5.68,0,0,0-6.839-.769,5.663,5.663,0,0,0-1.426,1.573v-2.048H98.674q.036.841,0,7.717v6.728h4.791V651.8a3.592,3.592,0,0,1,.146-1.17,2.913,2.913,0,0,1,.878-1.206,2.429,2.429,0,0,1,1.609-.549,2.108,2.108,0,0,1,1.865.914,4.265,4.265,0,0,1,.549,2.341v7.752H113.3Z" fill="var(--footer-icons)"></path></g></g></svg></a></li><li class="social"><div class="terms"><p>Content Licensed Under CC-BY-NC-4.0</p><p>Code Samples and Exercises Licensed Under Apache 2.0</p><p>Site Designed by<!-- --> <a href="https://www.alexdanielson.com/">Alex Danielson</a></p></div></li></ul><div class="theme-icons"><button aria-label="Activate dark mode" title="Activate dark mode" class="theme-toggle"><svg xmlns="http://www.w3.org/2000/svg" width="36px" height="100%" viewBox="0 -960 960 960" fill="var(--text-footer)" role="img"><title>Dark Mode Icon</title><path d="M480-120q-150 0-255-105T120-480q0-150 105-255t255-105q14 0 27.5 1t26.5 3q-41 29-65.5 75.5T444-660q0 90 63 153t153 63q55 0 101-24.5t75-65.5q2 13 3 26.5t1 27.5q0 150-105 255T480-120Zm0-80q88 0 158-48.5T740-375q-20 5-40 8t-40 3q-123 0-209.5-86.5T364-660q0-20 3-40t8-40q-78 32-126.5 102T200-480q0 116 82 198t198 82Z"></path></svg></button></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"attributes":{"description":"Brian Holt explains practical options for running tool-enabled LLMs within the MCP framework, including Tome (an open-source chat app supporting OpenAI and Google Gemini) and Ollama for local model hosting, with guidance on selecting tool-capable models and managing GPU/CPU inference costs. It also highlights OpenRouter and the Tome Discord community for easy model switching within MCP.","keywords":["Tome","Ollama","tools calling","OpenRouter","MCP","AI agents","Brian Holt"]},"html":"\u003cp\u003eI wanted to give you a second option if for whatever reason you don\u0026#39;t want to use Claude Desktop. Again, there are a myriad of choices but I was particularly impressed with \u003ca href=\"https://gettome.app\"\u003eTome\u003c/a\u003e. Tome is an open source app to chat with LLMs. It supports OpenAI, Google Gemini, and Ollama - so if you want to use either OpenAI or Gemini, here\u0026#39;s the place to do it.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote that Tome, only (as of writing), supports tools, not prompts or resources. But honestly tools are the most important and fun anyway.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eYou can also run models locally on your computer using \u003ca href=\"https://ollama.com/\"\u003eOllama\u003c/a\u003e. Ollama is a very cool piece of software that makes it\u0026#39;s trivial to run models on your computer. It will spin up a local web server for you to make queries against. We can have our Tome then query that locally-running LLM.\u003c/p\u003e\n\u003cp\u003eKeep in mind that the larger, \u0026quot;smarter\u0026quot; models typically require beefy GPUs to run with any sort of speed. That said, there are some models that run on even just CPUs and can still do tools calling.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNew models are coming out at break-neck pace. By the time you read this versus when I wrote it, I guarantee there will be several cycle of new models available. You will have to select one based on what\u0026#39;s available. The good news is that the new ones are only getting more performant and \u0026quot;smarter\u0026quot;.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWhat\u0026#39;s important is that you select a model that can handle \u003ca href=\"https://ollama.com/search?c=tools\"\u003etools calling\u003c/a\u003e. As of writing, a few good options that run on less-capable hardware\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://ollama.com/library/deepseek-r1\"\u003eDeepSeek R1 1.5B\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ollama.com/library/qwen3\"\u003eQwen3 0.6B\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ollama.com/library/phi4-mini\"\u003ePhi4-Mini 3.8B\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ollama.com/library/llama3.2\"\u003eLlama 3.2 1B\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe B means billions, and it\u0026#39;s how many parameters that model has which (\u003cem\u003every roughly\u003c/em\u003e) can translate to how smart it is, sorta like how megahertz \u003cem\u003every roughly\u003c/em\u003e translates to how capable a CPU is – they\u0026#39;re loosely related but you shouldn\u0026#39;t say that the Llama model is smarter than the DeepSeek one just because it has more parameters.\u003c/p\u003e\n\u003cp\u003eYou may need to try a few models before you find one that works well with what you\u0026#39;re doing. Just make sure you\u0026#39;re using tools-calling models as not all of them do it. I at home run Ollama on my gaming PC so I can take advantage of the beefy GPU I have in there - you can definitely do that too since Ollama is just a web server. In that case you can run a larger model as it will fit in VRAM.\u003c/p\u003e\n\u003cp\u003eAgain, keep in mind that running inference on a model is very computationally expensive so you shouldn\u0026#39;t expose it to the world and you should be aware that if you run a lot of inference you can rack up a power bill!\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWith Tome it doesn\u0026#39;t show you streaming text while it\u0026#39;s thinking so if you picked a big model that\u0026#39;s a bit slower to run, it can look like it\u0026#39;s not doing anything.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eI asked the creators of Tome if they wanted to share anything with you all and they said if you\u0026#39;re passionate about LLMs, agents, and/or MCP servers to join them on \u003ca href=\"https://discord.com/invite/9CH6us29YA\"\u003eDiscord\u003c/a\u003e!\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI\u0026#39;ll put one shout out in here for \u003ca href=\"https://openrouter.ai/\"\u003eOpenRouter.ai\u003c/a\u003e. If you want to try a lot of models and make it easy to switch between them, OpenRouter makes that very simple to do. Lots of other services exist (including on Databricks!) but OpenRouter works very well in this use case.\u003c/p\u003e\n\u003c/blockquote\u003e\n","markdown":"\nI wanted to give you a second option if for whatever reason you don't want to use Claude Desktop. Again, there are a myriad of choices but I was particularly impressed with [Tome][tome]. Tome is an open source app to chat with LLMs. It supports OpenAI, Google Gemini, and Ollama - so if you want to use either OpenAI or Gemini, here's the place to do it.\n\n\u003e Note that Tome, only (as of writing), supports tools, not prompts or resources. But honestly tools are the most important and fun anyway.\n\nYou can also run models locally on your computer using [Ollama][ollama]. Ollama is a very cool piece of software that makes it's trivial to run models on your computer. It will spin up a local web server for you to make queries against. We can have our Tome then query that locally-running LLM.\n\nKeep in mind that the larger, \"smarter\" models typically require beefy GPUs to run with any sort of speed. That said, there are some models that run on even just CPUs and can still do tools calling.\n\n\u003e New models are coming out at break-neck pace. By the time you read this versus when I wrote it, I guarantee there will be several cycle of new models available. You will have to select one based on what's available. The good news is that the new ones are only getting more performant and \"smarter\".\n\nWhat's important is that you select a model that can handle [tools calling][tools]. As of writing, a few good options that run on less-capable hardware\n\n- [DeepSeek R1 1.5B][r1]\n- [Qwen3 0.6B][qwen]\n- [Phi4-Mini 3.8B][phi]\n- [Llama 3.2 1B][llama]\n\nThe B means billions, and it's how many parameters that model has which (_very roughly_) can translate to how smart it is, sorta like how megahertz _very roughly_ translates to how capable a CPU is – they're loosely related but you shouldn't say that the Llama model is smarter than the DeepSeek one just because it has more parameters.\n\nYou may need to try a few models before you find one that works well with what you're doing. Just make sure you're using tools-calling models as not all of them do it. I at home run Ollama on my gaming PC so I can take advantage of the beefy GPU I have in there - you can definitely do that too since Ollama is just a web server. In that case you can run a larger model as it will fit in VRAM.\n\nAgain, keep in mind that running inference on a model is very computationally expensive so you shouldn't expose it to the world and you should be aware that if you run a lot of inference you can rack up a power bill!\n\n\u003e With Tome it doesn't show you streaming text while it's thinking so if you picked a big model that's a bit slower to run, it can look like it's not doing anything.\n\nI asked the creators of Tome if they wanted to share anything with you all and they said if you're passionate about LLMs, agents, and/or MCP servers to join them on [Discord][discord]!\n\n\u003e I'll put one shout out in here for [OpenRouter.ai][openrouter]. If you want to try a lot of models and make it easy to switch between them, OpenRouter makes that very simple to do. Lots of other services exist (including on Databricks!) but OpenRouter works very well in this use case.\n\n[tome]: https://gettome.app\n[discord]: https://discord.com/invite/9CH6us29YA\n[ollama]: https://ollama.com/\n[tools]: https://ollama.com/search?c=tools\n[qwen]: https://ollama.com/library/qwen3\n[r1]: https://ollama.com/library/deepseek-r1\n[phi]: https://ollama.com/library/phi4-mini\n[llama]: https://ollama.com/library/llama3.2\n[openrouter]: https://openrouter.ai/\n","slug":"tome-and-ollama","title":"Tome and Ollama","section":"Setup MCP Clients","icon":"computer","filePath":"/home/runner/work/complete-intro-to-mcp/complete-intro-to-mcp/lessons/03-setup-mcp-clients/B-tome-and-ollama.md","nextSlug":"/lessons/setup-mcp-clients/other-clients","prevSlug":"/lessons/setup-mcp-clients/claude-desktop"}},"__N_SSG":true},"page":"/lessons/[section]/[slug]","query":{"section":"setup-mcp-clients","slug":"tome-and-ollama"},"buildId":"3Mg5_eXgxRUcnLOJp0h8Y","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>